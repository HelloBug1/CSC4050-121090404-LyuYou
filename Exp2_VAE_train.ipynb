{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_channels=30):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=4, stride=4, padding=0) # 64*64\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1)  # 32*32\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # 16*16\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # 8*8\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Fully-connected layers for the bottleneck\n",
    "        self.fc_mu = nn.Linear(128 * 8 * 8, latent_channels)\n",
    "        self.fc_logvar = nn.Linear(128 * 8 * 8, latent_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(-1, 128 * 8 * 8)  # Flatten the output\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_channels=30):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_channels, 128 * 8 * 8)  # Match the output size of Encoder's last layer\n",
    "        \n",
    "        # Deconvolutional blocks\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # 16x16\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # 32x32\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1)  # 64x64\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.deconv4 = nn.ConvTranspose2d(16, 8, kernel_size=4, stride=4, padding=0)  # 256x256\n",
    "        self.bn4 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        # Output layers\n",
    "        self.out1 = nn.ConvTranspose2d(8, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.out2 = nn.ConvTranspose2d(8, 3, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, 8, 8)  # Reshape for deconvolution\n",
    "        x = F.relu(self.bn1(self.deconv1(x)))\n",
    "        x = F.relu(self.bn2(self.deconv2(x)))\n",
    "        x = F.relu(self.bn3(self.deconv3(x)))\n",
    "        x = F.relu(self.bn4(self.deconv4(x)))\n",
    "        return torch.sigmoid(self.out1(x)), torch.sigmoid(self.out2(x))\n",
    "\n",
    "\n",
    "class Exp2VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_channels=30):\n",
    "        super(Exp2VariationalAutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_channels=latent_channels)\n",
    "        self.decoder = Decoder(latent_channels=latent_channels)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# BraTS2020 Data files structure\n",
    "# data\n",
    "# ├── test\n",
    "# │   ├── healthy_test_images.npy\n",
    "# │   ├── unhealthy_test_images.npy\n",
    "# ├── |── unhealthy_test_masks.npy\n",
    "# ├── train\n",
    "# │   ├── healthy_train_images.npy\n",
    "\n",
    "class BraTS2020Dataset(Dataset):\n",
    "    def __init__(self, root_dir, train=True, healthy=True, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            train (bool): True to load training data, False for test data.\n",
    "            healthy (bool): True to load healthy images, False for unhealthy.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.healthy = healthy\n",
    "        self.transform = transform\n",
    "        self.data = None\n",
    "        self.load_data(root_dir)\n",
    "\n",
    "    def load_data(self, root_dir):\n",
    "        # Construct file path based on the provided arguments\n",
    "        data_type = 'train' if self.train else 'test'\n",
    "        health_status = 'healthy' if self.healthy else 'unhealthy'\n",
    "        file_name = f'{health_status}_{data_type}_images.npy'\n",
    "        print(f\"Loading {data_type} {health_status} data from {file_name}\")\n",
    "        \n",
    "        # Load the data\n",
    "        file_path = f'{root_dir}/{data_type}/{file_name}'\n",
    "        print(f\"Loading data from: {file_path}\")\n",
    "        self.data = np.load(file_path)\n",
    "\n",
    "        # Check the data shape\n",
    "        print(f\"Loaded {data_type} data with shape: {self.data.shape}\")\n",
    "        \n",
    "        # For unhealthy test data, we could also load masks here if needed\n",
    "        if not self.train and not self.healthy:\n",
    "            mask_file_path = f'{root_dir}/{data_type}/unhealthy_test_masks.npy'\n",
    "            self.masks = np.load(mask_file_path)\n",
    "        else:\n",
    "            self.masks = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        # Check the image shape\n",
    "        # print(f\"Image shape: {image.shape}\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {'image': image}\n",
    "        \n",
    "        # Include masks in the sample if this is unhealthy test data\n",
    "        if self.masks is not None:\n",
    "            mask = self.masks[idx]\n",
    "            if self.transform:\n",
    "                mask = self.transform(mask)\n",
    "            sample['mask'] = mask\n",
    "\n",
    "        return sample\n",
    "\n",
    "def load_dataset(data_dir, dataset_class, batch_size=32, train=True, healthy=True, shuffle=True):\n",
    "    # Define the transforms\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Pad((8, 8), fill=0, padding_mode='constant'),\n",
    "    ])\n",
    "    # Load the dataset\n",
    "    dataset = dataset_class(data_dir, train=train, healthy=healthy, transform=data_transforms)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=True)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_val_dataloader(dataset, fold_idx, train_batch_size, val_batch_size, k=5):\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    val_split = int(np.floor(len(dataset) / k))\n",
    "    train_indices, val_indices = indices[:fold_idx * val_split] + indices[(fold_idx + 1) * val_split:], indices[fold_idx * val_split:(fold_idx + 1) * val_split]\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    del dataset\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def qr_loss(x, q1, q2, mu, logvar, qval1=0.15, qval2=0.5):\n",
    "    # Quantile Regression Loss\n",
    "    q1_loss = torch.sum(torch.max(qval1 * (x - q1), (qval1 - 1) * (x - q1)))\n",
    "    q2_loss = torch.sum(torch.max(qval2 * (x - q2), (qval2 - 1) * (x - q2)))\n",
    "    recon_loss = q1_loss + q2_loss\n",
    "    kld_loss = -torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld_loss\n",
    "\n",
    "def calculate_rejection_mask(images, mean_recon, std_recon, output_size, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the rejection mask for each image based on the reconstruction error,\n",
    "    rejecting a pixel if two or more channels are rejected.\n",
    "\n",
    "    Args:\n",
    "    images (Tensor): The original images tensor.\n",
    "    mean_recon (Tensor): The mean of the reconstructed images.\n",
    "    std_recon (Tensor): The standard deviation of the reconstruction errors.\n",
    "    output_size (tuple): The spatial size of the output images.\n",
    "    threshold (float): The significance level for rejecting the null hypothesis.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A tensor of rejection masks shaped according to output_size.\n",
    "    \"\"\"\n",
    "    # Adjust standard deviation to avoid division by zero\n",
    "    std_recon_adjusted = torch.where(std_recon == 0, torch.tensor(1e-8), std_recon)\n",
    "    \n",
    "    # Calculate z-scores\n",
    "    z_scores = ((images - mean_recon) / std_recon_adjusted).to('cpu').numpy()\n",
    "    \n",
    "    # Calculate p-values based on z-scores\n",
    "    p_values = (2 * norm.sf(np.abs(z_scores))).flatten()\n",
    "    \n",
    "    # Apply multiple testing correction\n",
    "    reject, _, _, _ = multipletests(p_values, alpha=threshold, method='fdr_bh')\n",
    "    \n",
    "    # Reshape rejection results to match the original image dimensions\n",
    "    reject_mask = reject.reshape(images.shape[0], 3, *output_size)\n",
    "    \n",
    "    # Count the number of rejected channels per pixel\n",
    "    reject_mask = np.sum(reject_mask, axis=1) >= 2\n",
    "    reject_mask = reject_mask[:, np.newaxis, :, :]  # Add an axis for compatibility with image data format\n",
    "\n",
    "    return reject_mask\n",
    "\n",
    "def train_qr_vae_incremental(\n",
    "        device,\n",
    "        model,\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        optimizer, \n",
    "        epochs=10, \n",
    "        save_checkpoint=True, \n",
    "        checkpoint_dic_path='checkpoints/', \n",
    "        checkpoint_range=(100, 200), \n",
    "        checkpoint_interval=1, \n",
    "        single_transfer=True\n",
    "    ):\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    checkpoint_path = f'{checkpoint_dic_path}/checkpoint.pt'\n",
    "    losses_path = f'{checkpoint_dic_path}/losses.pt'\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer's state_dict to the correct device after loading\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"Resuming training from epoch {start_epoch} with best loss {best_val_loss:.4f}\")\n",
    "        if os.path.exists(losses_path):\n",
    "            losses_checkpoint = torch.load(losses_path)\n",
    "            train_losses = losses_checkpoint['train_losses'][0:start_epoch]\n",
    "            val_losses = losses_checkpoint['val_losses'][0:start_epoch]\n",
    "            avg_val_loss = val_losses[-1]\n",
    "            print(f\"Loaded losses from file.\")\n",
    "\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    print(\"Training started.\")\n",
    "    for epoch in (pb := tqdm(range(start_epoch, epochs))):\n",
    "        model.train()\n",
    "        for train_sample in train_loader:\n",
    "            if not single_transfer or epoch == 0:\n",
    "                train_data = train_sample['image'].to(device)\n",
    "            # Training\n",
    "            with autocast():  # Run model inference in mixed precision\n",
    "                recon_batch, mu, logvar = model(train_data)\n",
    "                train_loss = qr_loss(train_data, recon_batch[0], recon_batch[1], mu, logvar)\n",
    "\n",
    "            scaler.scale(train_loss).backward()  # Scale the loss before backpropagation\n",
    "            scaler.step(optimizer)  # Update model parameters\n",
    "            scaler.update()  # Update the scaler\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            avg_train_loss = train_loss.item() / len(train_data)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            avg_val_loss = 0\n",
    "            data_length = 0\n",
    "            for val_sample in val_loader:\n",
    "                if not single_transfer or epoch == 0:\n",
    "                    val_data = val_sample['image'].to(device)\n",
    "                # Validation\n",
    "                data_length += len(val_data)\n",
    "                with autocast():\n",
    "                    avg_val_loss += validate_qr_vae_pinball_loss(model, val_data)\n",
    "            avg_val_loss /= data_length\n",
    "        \n",
    "        pb.set_postfix(train_loss=f'{avg_train_loss:.4f}', val_loss=f'{avg_val_loss:.4f}')\n",
    "\n",
    "        # Checkpoint saved if the loss is improved\n",
    "        if save_checkpoint and avg_val_loss < best_val_loss and epoch >= checkpoint_range[0] and epoch <= checkpoint_range[1] and epoch % checkpoint_interval == 0:\n",
    "            best_val_loss = avg_val_loss\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            torch.save({'train_losses': train_losses, 'val_losses': val_losses}, losses_path)\n",
    "            # print(\"Checkpoint saved.\")\n",
    "        val_losses.append(best_val_loss)\n",
    "\n",
    "    # Save the losses\n",
    "    torch.save({'train_losses': train_losses, 'val_losses': val_losses}, losses_path)\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "def train_qr_vae_incremental_kfold(\n",
    "        device, \n",
    "        dataset, \n",
    "        epochs=10, \n",
    "        train_batch_size=32, \n",
    "        val_batch_size=32,\n",
    "        k_fold = 5, \n",
    "        save_checkpoint=True, \n",
    "        checkpoint_dic_path='checkpoints/', \n",
    "        checkpoint_ranges=None, \n",
    "        checkpoint_interval=1,\n",
    "        single_transfer=True\n",
    "    ):\n",
    "    os.makedirs(checkpoint_dic_path, exist_ok=True)\n",
    "    for fold_idx in range(k_fold):\n",
    "        train_loader, val_loader = get_train_val_dataloader(dataset, fold_idx, train_batch_size=train_batch_size, val_batch_size=val_batch_size, k=5)\n",
    "\n",
    "        model = Exp2VariationalAutoEncoder()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        # Clear the gradients before training\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        checkpoint_dic_path_fold = f'{checkpoint_dic_path}/fold_{fold_idx}'\n",
    "        os.makedirs(checkpoint_dic_path_fold, exist_ok=True)\n",
    "\n",
    "        if checkpoint_ranges[fold_idx] == None:\n",
    "            checkpoint_ranges[fold_idx] = (0, epochs)\n",
    "\n",
    "        train_qr_vae_incremental(\n",
    "            device, \n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            optimizer, \n",
    "            epochs, \n",
    "            save_checkpoint, \n",
    "            checkpoint_dic_path_fold, \n",
    "            checkpoint_ranges[fold_idx], \n",
    "            checkpoint_interval,\n",
    "            single_transfer\n",
    "        )\n",
    "    \n",
    "def validate_qr_vae_pinball_loss(model, val_data):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # recon_batch is expected to contain quantiles and mean as follows:\n",
    "        # recon_batch[0] = 0.15 quantile\n",
    "        # recon_batch[1] = median\n",
    "        recon_batch, mu, logvar = model(val_data)\n",
    "        val_loss = qr_loss(val_data, recon_batch[0], recon_batch[1], mu, logvar)\n",
    "        val_loss = val_loss.item()\n",
    "        \n",
    "        return val_loss\n",
    "\n",
    "def load_and_plot_losses(start, end, checkpoint_dic_path='checkpoints/', k_fold=5):\n",
    "    for fold_idx in range(k_fold):\n",
    "        checkpoint_dic_path_fold = f'{checkpoint_dic_path}/fold_{fold_idx}'\n",
    "        losses_path = f'{checkpoint_dic_path_fold}/losses.pt'\n",
    "        checkpoint = torch.load(losses_path)\n",
    "        train_losses = checkpoint['train_losses'][start:end]\n",
    "        val_losses = checkpoint['val_losses'][start:end]\n",
    "        # Print the best validation loss and the corresponding epoch\n",
    "        print(f\"Fold {fold_idx} Best Validation Loss: {min(val_losses):.4f} at Epoch {np.argmin(val_losses) + start}\")\n",
    "        # Plot the losses in different subplots\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Train Loss')\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(val_losses, label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading train healthy data from healthy_train_images.npy\n",
      "Loading data from: data/train/healthy_train_images.npy\n",
      "Loaded train data with shape: (1355, 240, 240, 3)\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Pad((8, 8), fill=0, padding_mode='constant'),\n",
    "])\n",
    "dataset = BraTS2020Dataset(data_dir, train=True, healthy=True, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/5000 [00:10<2:47:31,  2.01s/it, train_loss=111000.5484, val_loss=109622.8870]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_FOLDS):\n\u001b[0;32m      6\u001b[0m     checkpoint_ranges\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;241m1000\u001b[39m, NUM_EPOCHS))\n\u001b[1;32m----> 7\u001b[0m train_qr_vae_incremental_kfold(\n\u001b[0;32m      8\u001b[0m     device, \n\u001b[0;32m      9\u001b[0m     dataset, \n\u001b[0;32m     10\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS, \n\u001b[0;32m     11\u001b[0m     train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m,\n\u001b[0;32m     12\u001b[0m     val_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     13\u001b[0m     k_fold \u001b[38;5;241m=\u001b[39m NUM_FOLDS,\n\u001b[0;32m     14\u001b[0m     save_checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     15\u001b[0m     checkpoint_dic_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints/\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     16\u001b[0m     checkpoint_ranges\u001b[38;5;241m=\u001b[39mcheckpoint_ranges, \n\u001b[0;32m     17\u001b[0m     checkpoint_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m     18\u001b[0m     single_transfer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 277\u001b[0m, in \u001b[0;36mtrain_qr_vae_incremental_kfold\u001b[1;34m(device, dataset, epochs, train_batch_size, val_batch_size, k_fold, save_checkpoint, checkpoint_dic_path, checkpoint_ranges, checkpoint_interval, single_transfer)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_ranges[fold_idx] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m     checkpoint_ranges[fold_idx] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, epochs)\n\u001b[1;32m--> 277\u001b[0m train_qr_vae_incremental(\n\u001b[0;32m    278\u001b[0m     device, \n\u001b[0;32m    279\u001b[0m     model, \n\u001b[0;32m    280\u001b[0m     train_loader, \n\u001b[0;32m    281\u001b[0m     val_loader, \n\u001b[0;32m    282\u001b[0m     optimizer, \n\u001b[0;32m    283\u001b[0m     epochs, \n\u001b[0;32m    284\u001b[0m     save_checkpoint, \n\u001b[0;32m    285\u001b[0m     checkpoint_dic_path_fold, \n\u001b[0;32m    286\u001b[0m     checkpoint_ranges[fold_idx], \n\u001b[0;32m    287\u001b[0m     checkpoint_interval,\n\u001b[0;32m    288\u001b[0m     single_transfer\n\u001b[0;32m    289\u001b[0m )\n",
      "Cell \u001b[1;32mIn[2], line 208\u001b[0m, in \u001b[0;36mtrain_qr_vae_incremental\u001b[1;34m(device, model, train_loader, val_loader, optimizer, epochs, save_checkpoint, checkpoint_dic_path, checkpoint_range, checkpoint_interval, single_transfer)\u001b[0m\n\u001b[0;32m    205\u001b[0m     recon_batch, mu, logvar \u001b[38;5;241m=\u001b[39m model(train_data)\n\u001b[0;32m    206\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m qr_loss(train_data, recon_batch[\u001b[38;5;241m0\u001b[39m], recon_batch[\u001b[38;5;241m1\u001b[39m], mu, logvar)\n\u001b[1;32m--> 208\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(train_loss)\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Scale the loss before backpropagation\u001b[39;00m\n\u001b[0;32m    209\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)  \u001b[38;5;66;03m# Update model parameters\u001b[39;00m\n\u001b[0;32m    210\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()  \u001b[38;5;66;03m# Update the scaler\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lvyou\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lvyou\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start saving checkpoints after epoch 2000\n",
    "NUM_EPOCHS = 5000\n",
    "NUM_FOLDS = 1\n",
    "checkpoint_ranges = []\n",
    "for fold_idx in range(NUM_FOLDS):\n",
    "    checkpoint_ranges.append((1000, NUM_EPOCHS))\n",
    "train_qr_vae_incremental_kfold(\n",
    "    device, \n",
    "    dataset, \n",
    "    epochs=NUM_EPOCHS, \n",
    "    train_batch_size=240,\n",
    "    val_batch_size=32,\n",
    "    k_fold = NUM_FOLDS,\n",
    "    save_checkpoint=True, \n",
    "    checkpoint_dic_path='checkpoints/', \n",
    "    checkpoint_ranges=checkpoint_ranges, \n",
    "    checkpoint_interval=200,\n",
    "    single_transfer=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_plot_losses(90, NUM_EPOCHS, checkpoint_dic_path='checkpoints/', k_fold=NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_idx = 0\n",
    "test_data_loader = load_dataset(data_dir, BraTS2020Dataset, train=False, healthy=True, batch_size=1)\n",
    "sample = next(iter(test_data_loader))\n",
    "for checkpoint_path_idx in range(NUM_FOLDS):\n",
    "    checkpoint_path = f'checkpoints/fold_{checkpoint_path_idx}/checkpoint.pt'\n",
    "\n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = Exp2VariationalAutoEncoder().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    (\"Model loaded from checkpoint.\")\n",
    "\n",
    "    # Test the model on one picture\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data = sample['image'].to(device)\n",
    "        (q1, q2), mu, logvar = model(test_data)\n",
    "        # Plot the original image\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original\")\n",
    "        # show the first channel of the image\n",
    "        first_channel = test_data.cpu().numpy().squeeze()[0]\n",
    "        plt.imshow(first_channel, cmap='gray')\n",
    "        # Plot the reconstructed image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Reconstruction (0.15 quantile)\")\n",
    "        first_channel = q1.cpu().numpy().squeeze()[0]\n",
    "        plt.imshow(first_channel, cmap='gray')\n",
    "        # Plot the reconstructed image\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Reconstruction (median)\")\n",
    "        first_channel = q2.cpu().numpy().squeeze()[0]\n",
    "        plt.imshow(first_channel, cmap='gray')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qr_vae(device, model, test_loader, output_size=(256, 256), healthy=True, threshold=0.05):\n",
    "    model.eval()\n",
    "    rejection_rates = []\n",
    "    true_positives, actual_positives = 0, 0  # Initialize counters for unhealthy mode\n",
    "\n",
    "    # Iterate through the test data\n",
    "    for data in test_loader:\n",
    "        images = data['image'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            recon_batch, _, _ = model(images)\n",
    "            mean_recon = recon_batch[1]\n",
    "            std_recon = mean_recon - recon_batch[0]  # Standard deviation approximation\n",
    "            std_recon = torch.where(std_recon == 0, torch.tensor(1e-8), std_recon)\n",
    "            reject = calculate_rejection_mask(images, mean_recon, std_recon, output_size, threshold=0.05)\n",
    "            print(f\"Size of reject: {reject.shape}\")\n",
    "\n",
    "        if not healthy:\n",
    "            masks = data['mask'].numpy().astype(bool)  # Ensure boolean type for accuracy calculation\n",
    "            masks.reshape(masks.shape[0], masks.shape[2], masks.shape[3])\n",
    "            print(f\"Shape of masks: {masks.shape}\")\n",
    "            \n",
    "            # Calculate true positives and actual positives\n",
    "            true_positives += np.sum(masks & reject)  # Intersection of truth and prediction\n",
    "            actual_positives += np.sum(masks)\n",
    "        else:\n",
    "            rejection_rate = np.mean(reject)\n",
    "            rejection_rates.append(rejection_rate)\n",
    "\n",
    "    if not healthy:\n",
    "        average_metric = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "        print(f\"Average accuracy for unhealthy data: {average_metric:.4f}\")\n",
    "    else:\n",
    "        average_metric = np.mean(rejection_rates)\n",
    "        print(f\"Average rejection rate for healthy data: {average_metric:.4f}\")\n",
    "\n",
    "    return average_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "healthy_test_loader = load_dataset(data_dir, BraTS2020Dataset, train=False, healthy=True, batch_size=1024)\n",
    "unhealthy_test_loader = load_dataset(data_dir, BraTS2020Dataset, train=False, healthy=False, batch_size=1024)\n",
    "for checkpoint_path_idx in range(NUM_FOLDS):\n",
    "    print(f\"Fold {checkpoint_path_idx}\")\n",
    "    checkpoint_path = f'checkpoints/fold_{checkpoint_path_idx}/checkpoint.pt'\n",
    "\n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = Exp2VariationalAutoEncoder().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    (\"Model loaded from checkpoint.\")\n",
    "    test_qr_vae(device, model, healthy_test_loader, healthy=True)\n",
    "    test_qr_vae(device, model, unhealthy_test_loader, healthy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_idx = 0\n",
    "test_data_loader = load_dataset(data_dir, BraTS2020Dataset, train=False, healthy=False, batch_size=1, shuffle=True)\n",
    "sample = next(iter(test_data_loader))\n",
    "for checkpoint_path_idx in range(NUM_FOLDS):\n",
    "    checkpoint_path = f'checkpoints/fold_{checkpoint_path_idx}/checkpoint.pt'\n",
    "\n",
    "    # Load the best model\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = Exp2VariationalAutoEncoder().to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    (\"Model loaded from checkpoint.\")\n",
    "\n",
    "    # Test the model on one picture\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_data = sample['image'].to(device)\n",
    "        (q1, q2), mu, logvar = model(test_data)\n",
    "        # Plot the original image\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original\")\n",
    "        # show the first channel of the image\n",
    "        first_channel = test_data.cpu().numpy()[0, 0]\n",
    "        plt.imshow(first_channel, cmap='gray')\n",
    "        # Plot the ground truth mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        mask = sample['mask'].numpy()[0, 0]\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        # Plot the rejection mask\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Rejection Mask\")\n",
    "        reject_mask = calculate_rejection_mask(test_data, q2, q2 - q1, (256, 256))[0].squeeze()\n",
    "        plt.imshow(reject_mask, cmap='gray')\n",
    "        plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
